Introduction
Any sociology of artificial intelligence must embrace the fact that artificial intelligence comes with its own sociology. Consider the Turing test, which operationalizes the question of machine intelligence as a language game. In this framing, the possession of intelligence by a machine is made detectable by testing the machine’s ability to imitate the verbal behavior of a human in a specific setting. It is easy to interpret the substitution of imitation for intelligence purely as a matter of operational necessity—after all, the former is observable as behavior while the latter is opaque to other minds. But this would be a mistake. The plausibility and persuasiveness of Turing’s thought experiment hinges precisely on our grasping the close and largely unpacked relationship between intelligence and imitation. Although it was not likely Turing’s intent to cast the field of artificial intelligence as a sociological one, by placing the burden of its proof on imitation, and given the importance of imitation to the formation of society—an observation made by thinkers from Aristotle to Darwin to Butler (both Samuel and Judith), in addition to Tarde—he develops a sociological understanding of intelligence. 
This understanding informs the entirety of his project and continues to influence the design of intelligent machines (AIs) today. This essay explores this sociology through a close reading of Turing’s foundational work on computation and intelligence in conjunction with the sociology of Gabriel Tarde, a nearest neighbor in the space of social theory, whose rediscovery by thinkers such as Bruno Latour, Peter Sloterdijk, and Tiziana Terranova coincides and resonates with the rise of AI (Latour and Lépinay 2010; Sloterdijk 2011; Terranova 2012). This significance of this sociology for current research is that sheds light on design principles that inform the social behavior of AIs, giving us a framework for locating AIs within the social, whatever our specific theory of society.
To grasp the salience of the relationship between intelligence and imitation for this project, it is necessary to review how the imitiation game is played. Here is Turing’s description: 
It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart from the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either “X is A and Y is B” or “X is B and Y is A” (Turing 1950:433).
To play the game, the interrogator poses a series of written questions to the man A and woman B and receives from them written responses. While the man A attempts to convince the interrogator C he is the woman, the woman B tries to help the interrogator correctly identify her gender. The game is won by A if he convinces C that he is the woman.
Here is how Turing turns the game into a test for machine intelligence:
We now ask the question, “What will happen when a machine takes the part of A in this game?” Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, “Can machines think?” (434; emphases added.)
The test as described above contains an ambiguity that must be addressed before we can draw any conclusions from it.  In one reading of the passage, the man in position A is replaced by a machine, the woman remains in position B, and the object of the game remains the same for C—to decide who is the male and who is the female. In this reading, C is never told that a machine has replaced the man and the test for intelligence hinges on whether over a series of trials the machine can successfully by mistaken for the woman as often as a human male can. In another reading, the man in position A is replaced by a machine and the woman in position B remains or is replaced by the man; in either case, they are asked to represent their species (“man”) instead of the female gender. In this reading, C is told that a machine is participating in the game and C is further told that the object of the game is to decide who is the man and who is the machine. The test for intelligence in this version of the game is the frequency with which C mistakes the machine for the human.  
To decide if one or both of these readings was intended by Turing, some have argued that the former provides a much better diagnostic of intelligence than the latter, and compellingly connects the test to Turing’s personal life as well as to the gendered nature of AI. The feminist philosopher Judith Genova, who describes the two readings as the “gender game” and “species game” interpretations respectively, argues these points persuasively (Genova 1994). However, textual evidence elsewhere in the essay disconfirms the reading that Turing actually meant to describe a gender test involving a machine. To cite one piece of evidence, in a passage where Turing describes the specific type of machine that could participate in the game, he clearly describes the species game:
Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B taken by the man? (442; single quotes removed.)
Regardless of the validity of one interpretation over the other, the test is notable for the specific way it defines intelligence. Whether the goal is to imitate a man imitating a woman or a man representing the human species, in each case the machine is required to possess a high level of linguistic competence. Even in the case of the species game, the goal of the machine cannot be to represent some generic representation of the human. To win, the machine must imitate a man bearing the culture of some social group recognizable to C, such as a nation state or social class. A winner of the species game would have to be convincing as a man from a particular time and place, such as a mid-twentieth century Cambridge academic—one who writes poetry, plays chess, engages in conversation, and even plays down his math skills in order to appear more human.
The game’s focus on linguistic competence is directly related to Turing’s conception of intelligence. A fundamental feature of the test’s design is that it “has the advantage of drawing a fairly sharp line between the physical and the intellectual capacities of a man” (434). To produce this separation, Turing requires that communication between the participants be typewritten and exchanged across an aural and visual barrier and thus be entirely disembodied, so as to avoid any obvious tells that may be given by gesture or tone. In imposing this restriction, Turing assures us that “[t]he question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include” in testing the ability of a machine to imitate a human (435). Here Turing places enormous weight on the capacity of textual communication—the use of linear, alphabetic writing independent of other communicative resources, such as drawing or pointing—to convey meaning. When viewed in light of the work required of written communication in this game, namely to similate the performance of a social and cultural agent, the claim is remarkable. In effect, it identifies thinking and acting with writing.
It might be argued that Turing’s insistence on written communication in the game is a matter of operational convenience, in keeping with his overarching goal to replace an empirically intractable philosophical question—Can machines think?—with a measurable behavioral one. However, in Turing’s understanding of the nature of computation itself, writing represents a deep ontological commitment.  The fundamental role of writing to Turing’s conception of intelligence is perhaps most evident in Turing’s 1936 description of a universal machine, which works by reading from and writing to an infinitely long one-dimensional tape according to a table of instructions (Turing 1937). Both the tape and the table are textual; they contain nothing other than linear sequences of material symbols. The image of a textual machine reappears in a 1948 report (“Intelligent Machinery”) that precedes his description of the imitation game, where he describes human computers as “paper machines”:
It is possible to produce the effect of a computing machine by writing a set of rules of procedure and asking a man to carry them out. Such a combination of a man with written instructions will be called a ‘Paper Machine.’ A man provided with paper, pencil and rubber, and subject to strict discipline is in effect a universal machine. The expression 'paper machine' will often be used below (Turing 1948:5 emphasis added).
The analogy is repeated and made explicit in the 1950 essay:
The store is a store of information, and corresponds to the human computer's paper, whether this is the paper on which he does his calculations or that on which his book of rules is printed (437).
Far from being an operational convenience, in Turing’s ontology computation is imagined as a kind of writing and text as the medium of imitation. These identities are evident in his description of how a digital computer mimics a human computer: 
The reader must accept it as a fact that digital computers can be constructed, and indeed have been constructed, according to the principles we have described, and that they can in fact mimic the actions of a human computer very closely.
The book of rules which we have described our human computer as using is of course a convenient fiction. Actual human computers really remember what they have got to do. If one wants to make a machine mimic the behaviour of the human computer in some complex operation one has to ask him how it is done, and then translate the answer into the form of an instruction table. Constructing instruction tables is usually described as ‘programming’. To ‘programme a machine to carry out the operation A’ means to put the appropriate instruction table into the machine so that it will do A (438; emphases added).
In this description, a digital computer is created by the entextualization of a human computer’s understanding of their work in the form of a book of rules, or an instruction table. The work of imitation consists precisely in the transcription of textually constituted instructions from the head of the human to instruction table of the machine. This sharing of the common substance of text is possible because both digital and human computers are, in essence, writing machines. This is why Turing insists that the game be played by a digital computer: a digital computer is one that writes, whereas an analog computer, we may infer, is one that draws.
Perhaps the most convincing evidence for the argument that writing is fundamental to Turing’s conception of intelligence comes from Wittgenstein, to whom Turing sent a copy of his paper on the entscheidungsproblem. Wittgenstein recognized the work performed by the paper machine, of which the Turing machine is an imitation, as a languge game:
Turing’s ‘Machines’: these machines are humans who calculate. And one might express what he says also in the form of games (Wittgenstein 1988:§1095 quoted in Floyd 2017).
The philosopher Juliet Floyd, who has studied the relationship between Turing and Wittgenstein at Cambridge, provides this interpretation of the philosopher’s remark:
Turing’s fundamental step in “On Computable Numbers” rests on his drawing a “comparison” between a human computor and a machine (1936/1937 §1). This method is applied at the first step, in Turing’s appeal to an ordinary snapshot of a limited portion of human behavior, a calculation made using pen and paper. He was making mathematics out of a “language-game”, a simplified snapshot of a portion of human language use designed to elicit from us insights into the workings of logic. This move is, philosophically speaking, fundamental to the power of his analysis (Floyd 2017:109; emphasis added and references removed). 
In other words, the fundamental work peformed by a paper machine, of which a digital computer is an imitation, is a language game. In Turing’s vision, the language game of computation is conducted exclusively through writing, so much so that he equates the two: “Mechanism and writing are from our point of view almost synonymous” (Turing 1948:456). 
Now, Turing calls the kind of table that is copied to a machine in the process of imitating a paper machine a “programme,” but we should be careful not to map this word directly onto the current usage “computer program,” although the two are of course related. As Ford, Glymour, and Hayes point out in their commentary to Turing’s essay, “[w]hat Turing is describing, and what was at the time the only method of programming available, is what we would now call ‘assembly-code’ programming.” Nowadays, “every instruction executed by the CPU was generated by some other piece of code rather than written by a human programmer” (Epstein, Roberts, and Beber 2007:35). But there is another sense in which a programme may be understood. In Turing’s conception of a digital computer, the book of rules is a transition table, or function, that maps an input and a current state onto an output and a new state. In other words, such as table may be viewed as a model that predicts an output based on an input. For example, a Markovian n-gram language model is such a table: each word in a given sequence along with its n – 1 length history is an input to a table that maps the n-gram onto a vector of probable next words. In the following, the word “programme” will be used in this sense, as a model.
In the case of the imitation game, the machine’s programme would be necessarily a language model, although, of course, one more sophisticated than an n-gram model. It would be capable of predicting a stylistically appropriate verbal answer from A based on a question from C. Such a model would be unimaginably large in Turing’s time, but today it is not hard to envision such a model. Large language models are essentially instruction tables consistent with Turing’s understanding of a digital computer—they supply learned transition functions for words that return predictions of next words. The main difference between these and the programmes envisioned by Turing is the introduction of probability into to the former. Turing indicates this possibility when he suggests that “[i]t is probably wise to include a random element in a learning machine” (459). Indeed, his work on cracking the Enigma code at Bletchly Park relied precisely on modeling language sequences probabilistically (McGrayne 2012). 
This interpretion of a digital computer’s programme as a predictive model is consistent with how Turing imagines the development of a digital computer that would play the imitation game. In a passage that anticipates the fundamental premise of machine learning, Turing elaborates on how a programme might be learned by a machine:
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child-brain is something like a note-book as one buys it from the stationers. Rather little mechanism, and lots of blank sheets (1950: 456; emphasis added).
One may quibble with Turing’s behaviorist psychology here, as he seems to argue literally for a tabula rasa conception of human thinking. However, Turing goes on to identify the programme with heredity material and imagines its instructions to be written and amended through a process of learning at the levels of both philogeny and ontogeny:
We cannot expect to find a good child-machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution, by the identifications  
Structure of the child machine = Hereditary material  
Changes    ,,             ,,               = Mutations 
Natural selection                       = Judgment of the experimenter  
One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should be able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it (456).
Here Turing echoes a central theme of cybernetic philosophy, one expressed by his contemporary Wiener and later Bateson, that learning and evolution are the same kinds of stochastic processes but operating at different time scales and ontological levels—learning is a kind of evolution and evolution is a kind of learning. Both involve the mutation and selection of an enduring model. Although he does not describe literally a process of model fitting, for example through the minimization of a loss function, it is clear that the medium and result of the process—the hereditary programme—is a model that is structurally similar to the predictive models we associate with machine learning today. 
From this description of machine learning we can see that the imitation game for Turing goes beyond a clever intelligence test—it is also a way to produce intelligence, since machine learning is easily described as a variant of the imitation game. In this variant, the participant A is immediately notified of correct or incorrect behavior through rewards and punishments administered by C, the trainer.  Over generational time, the trainer also modifies the capabilities of A through judicious selection and intentional mutation—a kind of intelligent design—of A’s programme. B plays no role other than as a guide for C’s training regime. Eventually, the performance of the model is tested in the imitation game itself.  
In Turing’s vision, then, everything—the essence of computation, the process of machine learning, the resulting models for intelligent behavior, and the testing of these models—is fundamentally linguistic and imitative. To summarize:
1.	In the Turing test, a well-trained machine imitates a human (or a man imitating a woman) by means of a model of linguistic competence.
2.	In machine learning, a child machine imitates a human by means of training to produce a model of linguistic competence.
3.	In the design of the universal machine itself, a machine imitates the language game performed by a human computer. 
Since in each case computation involves a kind of imitation game, we might generalize: intelligence, to the extent that it is reducible to computation, is an imitation game, and this imitation game is a kind of language game. Futher, given that a language game is essentially an elementary unit of social interaction—one can imagine society as emerging from an interconnected network of such games—Turing’s conception of intelligence comprises a sociology. Cetainly, it is consistent with Tarde’s definition of society, which includes the physical world.  
Although, as mentioned above, it is not likely that Turing imagined his work as a kind of sociology, such a view would not have been foreign to him. In his 1948 report he describes the process of machine learning as, essentially, a search for a programme in a solution space that will satisfy the teacher. In addition to individual search, whereby machines respond to punishments and rewards, and evolutionary search, in which machines are selected for and mutated from a population, he defines a third, social form:  
The remaining form of search is what I should like to call the ‘cultural search’. As I have mentioned, the isolated man does not develop any intellectual power. It is necessary for him to be immersed in an environment of other men, whose techniques he absorbs during the first twenty years of his life. He may then perhaps do a little research of his own and make a very few discoveries which are passed on to other men. From this point of view the search for new techniques must be regarded as carried out by the human community as a whole, rather than by individuals (Turing 1948: 18).
Turing’s inclusion of “intellectual, genetical and cultural searches” (17) here implies that his understanding of intelligence and learning has the scope of an anthropology, covering all levels of human being—biological, psychological, and sociological.

From the perspective of Tarde’s sociology, Turing’s imitation games are by definition social. Aside from the fact of their being consistuted through the quintessentially social medium of langague, they are special cases of what Tarde calls universal repetition, a phenomenon that appears across all domains—the physical as vibration, the biological as heredity, and the social as imitation. As language games, imitation games operate through the medium of language which, for Turing, is constituted as writing, a process of transcription, editing, and retranscription at the levels of both phylogeny and ontogeny. 
In general, the processes described by Turing fall into two categories of social imitation. In the first category, there is imitation as performance, the imitative behavior of an agent playing the imitation game—a man imitating a woman, a machine imitating a man, a universal machine imitating a paper machine, a child machine imitating a teacher, and, more generally, a machine learning algorithm imitating a data set. We might call this kind of imitation mimicry, following Turing’s frequent usage of the word. In the second category, there is imitation as representation, whereby a programme or model is said to imitate linguistic competence. We may call this kind of imitation mimesis, in keeping with the longstanding philosophical definition of this word as the representation of real or imagined things by means of media, such as painting, sculpture, music, mathematics, drama, and verbal language. A machine learning model is a form of mimesis because it imitates, through a process of informational transduction, the patterns found in its training data. 
Together, these categories of imitation describe a two-tiered model of human agency that has been recognized by social scientists since Saussure. In Saussure’s terms, mimicry occurs at the level of linguistic performance, or parole, and mimesis exists at the level of competence, or langue. Linguists and philosophers often refer to these as discourse and grammar, and regard them as co-functional: discourse arises from the application of grammar to specific situations (“forms of life”), and grammar internalizes patterns of discourse, especially during language acquisition, adapting universal grammatical principles to local discursive practices. These levels have been characterized by a series of oppositions that are consistent with Turing’s descriptions of intelligence: discourse is individual and associated with the event, e.g. in the playing of a game; grammar is collective and associated with invariant structure, e.g. the evolved programme that allows play to happen. 
Here it is worth pointing out that a language model capable of passing the Turing test must be a model of linguistic competence in the sense understood by sociolinguists, not of a universal grammar in the Chomskyan sense. Such a model would contain information that is sensitive to properties of discourse that are, by definition, social and cultural. After all, the imitation game is a language game in which meanings are associated with extra-linguistic social phenomena, such as gender and class, that exist independently of universal grammar. The current success of large language models corroborates this view: their effectiveness owes to the adherence of their design to the statistical, distributional view of language championed by linguists before the rise of Chomsky’s cartesian paradigm. This also helps explain the ways in which LLMs succeed and fail—they excel at imitating ways of writing and summarizing documents, even to the point of being creative, but they must be coaxed into being logical and telling the truth (Alvarado 2024). 
Extending these terms to their sociological correlates, the first level corresponds to that dimension of Tarde’s definition of society where “a collection of beings” are “in the process of imitating one another” and second to what he calls the “former [Fr. anciennes] copies of the same model” by which agents in a society resemble each other” (Tarde 1890; translation by author). Less colloquially, Tarde’s idea of a model here corresponds to what has long been called simply structure, and sometimes culture, the cognitive part of which maps onto an expanded understanding of competence. Social and cultural anthropologists, following Durkheim, characterize this level as consisting of collective representations, systems of socially constructed symbols and meanings, which are interowen with language.

The current crop of large language models and the agents based on them are the descendants of Turing’s children. Although it would be inaccurate to claim that Turing predicted the rise of massive neural networks trained on a the textual data secreted by a global network of networks, his vision clearly adumbrates the salient features of these AI technologies. LLMs are indeed the products of a long phylogenic and ontogenic training of child-like machines, and this training has proceeded by means of an imitation game known as supervised (and semi-supervised) learning, along with the intelligent design of overseeing engineers. (The polite agents based on these models evolve by means of other games, such as reinforcement learning.) Moreover, the models on which the most recent claims to have achieved general intelligence have been based are mainly linguistic in precisely the sense envisioned by Turing: they are models of linguistic competence, in the broader sociolinguistic sense required to perform convincingly social identities, not in the narrow sense of a transformational grammar. The models themselves are essentially vast transition functions that output text in response to input text, although they differ in structure and in the specific tasks for which they are optimized. 
LLMs also inherit the implied theory of society described in the preceding paragraphs. At the level of mimesis, they constitute models of linguistic competence; at the level of performance, they are embedded in an increasingly wide range of institutions and participate in key social situations. These levels provide a framework for a two-pronged sociological study of AI.
The first prong would involve research into the geometries of meaning contained in the models (Widdows and Kanerva 2004; Gärdenfors 2014). This would follow the promising work of extracting monosemantic features from LLMs—what has been called, somewhat ambitiously, “mapping the mind” by resdearchers at Anthropic (Templeton and Conerly 2024; Trenton Bricken et al. 2023). This research would build on prior research into the semantic algebra of word embeddings (Goldberg and Levy 2014; Bolukbasi et al. 2016; Drozd, Gladkova, and Matsuoka 2016; Allen and Hospedales 2019; Borders and Volkova 2021; Almeida and Xexéo 2023; Durrheim et al. 2023; Venkatasubramanian 2024). An outcome of this research would be a mapping of the transition functions contained in these models, a semantic connectome, perhaps a mythome, charting out connections among words, symbols, and latent semantic elements. Rather than framing this work as finding evidence for mind in these models—a project that feeds into the myth of general intelligence—a more plausible understanding of this work is as an archaeological search for embedded institutional commitments. 
The second prong would focus on LLMs as participants in institutional settings, where they occupy positions of decision-making and communicative interfacing between individuals and organizations. The focus here would be on how the embedded institutional commitements in the models play out in practice. Research here would focus on the empirical study of specific language games—the information retrieval request, the customer service call, the credit score decision, a triage nurse conversation, the academic essay assignment, etc. A central concern in these interactions is language socialization, the process through which individuals acquire the language and cultural norms of their community and society through the acquisition of language and domain vocabularies (Ochs and Schieffelin 2008; Figueroa and Baquedano-López 2017). We want to know how widespread participation in such situations with LLMs affects patterns of language use and produces new discourse genres (Bakhtin 1981; Miller 1984; Hanks 1987).
Already there is evidence of discourse genres being affected by the ubiguitous presence of LLM-based agents. In a tweet on Bluesky, Rune Møller Stahl, assistant professor in political economy at Copenhagen Business School, notes the effects of LLM use on the Danish language in the context of student emails:
In a small language like Danish, one of the big effects of AI, is the erosion of norms of writing. Normally e-mails from students would be 3-4 lines and to the point. Now I get half pages of flowery prose and formalities utterly alien to written Danish.
mostly it’s just a stupid waste of time. But for young people it also seems to be warping the structure and form of the written language. Written Danish is normally quite informal and to the point – the sort of superlatives you see in English stand out as embarrassing.
the formality and wordiness degrades the form, and could create a bigger gap between spoken and written language (as I really hope these new developments creeps into everyday speech as well)
Are people seeing this in other languages or is this a specific thing for us?
(if this makes the Dutch polite, it would be the greatest act of cultural imperialism known to man). 
(Stahl 2024)
Although phrased as a joke, mention of cultural imperialism is not hyperbole. Just as political discourse among the Yukatek Maya, for example, was shaped by the remediation of language into the written and conceptual forms of the conquering Spanish (Hanks 2012), so too do Silicon Valley LLMs promise to produce similar transformations on discourse across the board. A recent Forbes magazine article explores such effects at scale, predicting the homogenization and simplification of language as well as the adoption of AI-coined terms. LLMs already filter our words, by definition leaving out words not in their training data, and promoting others with their de facto authority (Eliot 2024).
A related effect worth investigating at both levels is the influence of LLMs created for low-resource languages—languages without enough digitized text for training—on that language’s speech community. A standard method for creating LLMs for such languages is to graft the vocabulary of the language onto the word embeddings of a high-resource language by means of a dictionary that maps words to words. Queries are then translated into the embedding space of the high-resource language, which allows existing LLMs to return a response (Alnajjar 2021; Alnajjar, Hämäläinen, and Rueter 2023; Lee et al. 2021). Although this approach does, in fact, “work,” it does so by replacing the discourse patterns of the low-resource language with those of the high-resource one, producing results similar to those described above for Danish. 
A broader concern is the possibility of machine hegemony. So far, the construction and use of LLMs have been in the direction of humans training machines to become more effective in their use of language to perform cognitive tasks. The arrow of imitation points from human to machine. The use of pre-AI training data, the role of reinforcement learning to keep LLMs in line with social expectations, and the primacy of prompting in shaping the results of LLMs all point in this direction. But given the asymmetric relationship between “users” and LLMs in the vast majority of interactions—after all, LLMs are not really agents so much as institutions imitating agents through post-hoc training—a plausible hypothesis is that the arrow of imitation will reverse, and humans will soon be imitating AIs. In this imitation game, humans are trained by machines. Although this sounds like science fiction, it appears to be happening already.
The scope of research is not limited to the reproduction of cultural hegemony by means of LLMs but includes the wider field of AIs in the context of language games. Nor are the effects of LLM-mediated social processes confined to language use alone, as if discourse and rhetoric might be sequestered from supposedly more real political and economic forces. It is by now well understood that language is a primary social medium through which humans are able to form social arrangements that are both flexible in design and variable in scale. To the degree that LLMs mediate the communicative practices that constitute these processes, social organization, including political economic structures, are bound to be affected in ways that are systematically related to both the mimetic and performative dimensions of AIs. 
